{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning \n",
    "\n",
    "*[Link](https://arxiv.org/pdf/1808.00023.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anti-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decisions do not consider protected attributes.\n",
    "\n",
    "$$\n",
    "d(x)=d(x') \\: for \\, all \\: x, x'  \\: such \\, that \\: x_u =x'_u.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some given measure of classification error is equal across groups defined by the protected attributes.\n",
    "- Demographic parity: Parity in the proportion of positive decisions. Though this is not strictly a measure of \"error\", it can be included in classification parity since it can be computed from a confusion matrix.\n",
    "\n",
    "$$\n",
    "Pr(d(X)=1 \\, | \\, X_p) = Pr(d(X)=1)\n",
    "$$\n",
    "\n",
    "- Parity of false positive rates\n",
    "\n",
    "$$\n",
    "Pr(d(X)=1\\, |\\, Y=0, X_p) = Pr(d(X)=1\\,|\\,Y=0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcomes are independent of protected attributes conditional on risk score $s(x)$. \n",
    "\n",
    "$$\n",
    "Pr(Y=1\\,|\\,s(X), X_p) = Pr(Y=1\\,|\\,s(X))\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Formalizing Fairness in Prediction with Machine Learning\n",
    "*[Link](https://arxiv.org/pdf/1710.03184.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness through unawareness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor is said to achieve fairness through unawareness if protected attributes are not explicitly used in the prediction process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor $\\mathcal{H}$ is counterfactually fair, given $Z=z$ and $A=a$ for all $y$ and $a \\ne a'$, iff\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{\\mathcal{H}_{A=a} = y \\, | \\, Z = z, A=a \\} = \\mathbb{P}\\{\\mathcal{H}_{A=a'} = y \\, | \\, Z = z, A=a \\} \n",
    "$$\n",
    "\n",
    "\n",
    "This measure deems a predictor to be fair if its output remains the same when the protected attribute is flipped to its counterfactual value. This measure compares every individual with a different version of them- selves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group fairness (Statistical/Demographic parity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor $\\mathcal{H} : X \\to Y$ achieves group fairness with bias $\\epsilon$ with respect to groups $S$, $T \\subseteq X$, and $O \\subseteq A$ being any subset of outcomes iff\n",
    "\n",
    "$$\n",
    "\\mathopen| \\mathbb{P}\\{ \\mathbb{H}(x_i) \\in O \\, | \\, x_i \\in S \\} - \\mathbb{P}\\{ \\mathbb{H}(x_j) \\in O \\, | \\, x_j \\in T \\} \\mathclose|\n",
    "$$\n",
    "\n",
    "Group fairness imposes the condition of statistical and demographic parity on the predictor. Unlike some of the other formalizations of fairness, group fairness is independent of the “ground truth” i.e. the label information. This is useful when reliable ground truth information is not available e.g. in domains like employment, housing, credit and criminal justice, discrimination against protected groups has been well- documented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certifying and removing disparate impact\n",
    "*[Link](https://arxiv.org/pdf/1412.3756.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparate Impact (\"80% rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data set $D = (X, Y, C)$, with protected\n",
    "attribute $X$ (e.g., race, sex, religion, etc.), remaining attributes $Y$, and binary class to be predicted\n",
    "$C$ (e.g., “will hire”), we will say that $D$ has disparate impact if\n",
    "\n",
    "$$\\frac{Pr(C = YES\\,|\\,X = 0)}{Pr(C = YES\\,|\\,X = 1)} ≤ \\tau = 0.8$$\n",
    "\n",
    "for positive outcome class $YES$ and majority protected attribute $1$ where $Pr(C = c\\,|\\,X = x)$ denotes\n",
    "the conditional probability (evaluated over $D$) that the class outcome is $c \\in C$ given protected\n",
    "attribute $x \\in X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does mitigating ML’s impact disparity require treatment disparity?\n",
    "*[Link](https://papers.nips.cc/paper/8035-does-mitigating-mls-impact-disparity-require-treatment-disparity.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calders-Verwer (CV) gap and the p-% rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a given threshold $t$, let\n",
    "\n",
    "$$\n",
    "q_z = \\frac{1}{n_z}\\sum_{i:z_i=z}\\mathbb{1}(\\hat{p_i} > t) \\quad where \\; n_z = \\sum^n_i\\mathbb{1}(z_i=z)\n",
    "$$\n",
    "\n",
    "The Calders-Verwer (CV) gap $q_a - q_b$ is the difference between the proportions assigned to the positive calss in the advantaged group %a% and the disadvantaged group $b$. The p-% rule is a related metric. Classifiers satisfy the p-% rule if $q_b\\,/\\,q_a \\geq p\\,/\\,100$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
