{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning \n",
    "\n",
    "*[Link](https://arxiv.org/pdf/1808.00023.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anti-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decisions do not consider protected attributes.\n",
    "\n",
    "$$\n",
    "d(x)=d(x') \\: for \\, all \\: x, x'  \\: such \\, that \\: x_u =x'_u.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some given measure of classification error is equal across groups defined by the protected attributes.\n",
    "- Demographic parity: Parity in the proportion of positive decisions. Though this is not strictly a measure of \"error\", it can be included in classification parity since it can be computed from a confusion matrix.\n",
    "\n",
    "$$\n",
    "Pr(d(X)=1 \\, | \\, X_p) = Pr(d(X)=1)\n",
    "$$\n",
    "\n",
    "- Parity of false positive rates\n",
    "\n",
    "$$\n",
    "Pr(d(X)=1\\, |\\, Y=0, X_p) = Pr(d(X)=1\\,|\\,Y=0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcomes are independent of protected attributes conditional on risk score $s(x)$. \n",
    "\n",
    "$$\n",
    "Pr(Y=1\\,|\\,s(X), X_p) = Pr(Y=1\\,|\\,s(X))\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Formalizing Fairness in Prediction with Machine Learning\n",
    "*[Link](https://arxiv.org/pdf/1710.03184.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness through unawareness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor is said to achieve fairness through unawareness if protected attributes are not explicitly used in the prediction process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor $\\mathcal{H}$ is counterfactually fair, given $Z=z$ and $A=a$ for all $y$ and $a \\ne a'$, iff\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{\\mathcal{H}_{A=a} = y \\, | \\, Z = z, A=a \\} = \\mathbb{P}\\{\\mathcal{H}_{A=a'} = y \\, | \\, Z = z, A=a \\} \n",
    "$$\n",
    "\n",
    "\n",
    "This measure deems a predictor to be fair if its output remains the same when the protected attribute is flipped to its counterfactual value. This measure compares every individual with a different version of them- selves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group fairness (Statistical/Demographic parity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor $\\mathcal{H} : X \\to Y$ achieves group fairness with bias $\\epsilon$ with respect to groups $S$, $T \\subseteq X$, and $O \\subseteq A$ being any subset of outcomes iff\n",
    "\n",
    "$$\n",
    "\\mathopen| \\mathbb{P}\\{ \\mathbb{H}(x_i) \\in O \\, | \\, x_i \\in S \\} - \\mathbb{P}\\{ \\mathbb{H}(x_j) \\in O \\, | \\, x_j \\in T \\} \\mathclose|\n",
    "$$\n",
    "\n",
    "Group fairness imposes the condition of statistical and demographic parity on the predictor. Unlike some of the other formalizations of fairness, group fairness is independent of the “ground truth” i.e. the label information. This is useful when reliable ground truth information is not available e.g. in domains like employment, housing, credit and criminal justice, discrimination against protected groups has been well- documented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor achieves individual fiarness iff $\n",
    "\\mathcal{H}(x_i) \\approx \\mathcal{H}(x_j) \\, | \\, d(x_i, x_j) \\approx 0$ where $d : X \\times X \\to \\mathbb{R}$ is a distance metric for individuals.\n",
    "\n",
    "* In the original paper, it is stated as $\n",
    "\\mathcal{H}(x_i) \\approx \\mathcal{H}(x_j) \\, | \\, d(x_i, x_i) \\approx 0$ but it seems to be a typo. So I changed $d(x_i, x_i)$ to $d(x_i, x_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equality of opportunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor is said to satisfy equal opportunity with respect to group $S$ iff \n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{\\mathcal{H}(x_i) = 1 \\, | \\, y_i = 1, x_i \\in S \\} = \\mathbb{P}\\{\\mathcal{H}(x_j) = 1 \\, | \\, y_j = 1, x_j \\in X \\setminus S \\} \n",
    "$$\n",
    "\n",
    "It can be considered as a stipulation which states that the true positive rate should be the same for all the groups. An equivalent notion proposed by Zafar et al., called *disparate mistreatment*, asks for the equivalence of misclassification rates across the groups.\n",
    "\n",
    "* Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. 2017. Fairness Beyond Disparate Treatment &#38; Disparate Impact: Learning Classifica- tion Without Disparate Mistreatment. In Proceedings of the 26th International Conference on World Wide Web (WWW ’17). 1171–1180."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferred treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A group-conditional predictor is said to satisfy preferred treatment if each group of the population receives more benefit from their respective predictor then they would have received from any other predictor i.e.\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{B}_S(\\mathcal{H}_S) \\geq \\mathbb{B}_S(\\mathcal{H}_T) \\quad for \\, all \\ S, T \\subset X\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferred impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor $\\mathcal{H}$ is said to have preferred impact as compared to another predictor $\\mathcal{H}'$ if $\\mathcal{H}$ offers at-least as much benefit as $\\mathcal{H}'$ for all the groups.\n",
    "\n",
    "$$\n",
    "\\mathbb{B}_S(\\mathcal{H}_S) \\geq \\mathbb{B}_S(\\mathcal{H}') \\quad for \\, all \\ S \\subset X\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certifying and removing disparate impact\n",
    "*[Link](https://arxiv.org/pdf/1412.3756.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparate Impact (\"80% rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data set $D = (X, Y, C)$, with protected\n",
    "attribute $X$ (e.g., race, sex, religion, etc.), remaining attributes $Y$, and binary class to be predicted\n",
    "$C$ (e.g., “will hire”), we will say that $D$ has disparate impact if\n",
    "\n",
    "$$\\frac{Pr(C = YES\\,|\\,X = 0)}{Pr(C = YES\\,|\\,X = 1)} ≤ \\tau = 0.8$$\n",
    "\n",
    "for positive outcome class $YES$ and majority protected attribute $1$ where $Pr(C = c\\,|\\,X = x)$ denotes\n",
    "the conditional probability (evaluated over $D$) that the class outcome is $c \\in C$ given protected\n",
    "attribute $x \\in X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does mitigating ML’s impact disparity require treatment disparity?\n",
    "*[Link](https://papers.nips.cc/paper/8035-does-mitigating-mls-impact-disparity-require-treatment-disparity.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calders-Verwer (CV) gap and the p-% rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a given threshold $t$, let\n",
    "\n",
    "$$\n",
    "q_z = \\frac{1}{n_z}\\sum_{i:z_i=z}\\mathbb{1}(\\hat{p_i} > t) \\quad where \\; n_z = \\sum^n_i\\mathbb{1}(z_i=z)\n",
    "$$\n",
    "\n",
    "The Calders-Verwer (CV) gap $q_a - q_b$ is the difference between the proportions assigned to the positive calss in the advantaged group %a% and the disadvantaged group $b$. The p-% rule is a related metric. Classifiers satisfy the p-% rule if $q_b\\,/\\,q_a \\geq p\\,/\\,100$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
